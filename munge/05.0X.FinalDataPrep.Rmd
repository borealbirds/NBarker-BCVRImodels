---
title: "Final Data Preparation"
author: "Nicole Barker"
date: "Last run: Jan 3, 2018"
output: 
  word_document:
    reference_docx: ../styles/ReportFormat_1.docx
---

## Script Abstract

## Background
In previous steps, I filtered date out based on mismatch between VRI data and bird surveys. However, I still need to do a bit more data processing before I can do analyses. 


In this script: 

* Subset to desired behaviours
* I filter out any surveys with undesired survey duration intervals
* I filter out any surveys with undesired survey distance intervals
* I sum across species to develop a total bird count per survey 
* I explore relationships between project (PCODE) and apparent total bird count

**NOTE: I also plan to filter based on proximity of a point count to the stand edge (in a previous step), but I don't have the necessary information from Trish for that yet**

``` {r setup, echo=F, message=F, warning=F}
require(knitr)
opts_knit$set(root.dir = '..')
knitr::opts_chunk$set(
  fig.path = "figures/"
)
```

``` {r load.project, message=F}
require(ProjectTemplate)
load.project()
options(digits=12)
```

### Subset to columns needed for exploration of bird counts per different survey methods

``` {r}
load("cache/vri_bird_tempaligned.RData") #(PKEYs quality-checked for use)
colnames(vri_bird_tempaligned)
#load("cache/vri_bird_tempaligned-distfiltered.RData") # this doesn't exist yet

tmp <- read.csv("data/birddata_preprocess1.csv", header=T) # fuller bird dataset (with survey method details)

birddat <- merge(vri_bird_tempaligned[c("PKEY")], tmp, by="PKEY", all.x=T)
```

### Subset to desired behaviours 

Following Peter Solymos's guidelines (https://github.com/psolymos/bamanalytics/blob/master/R/dataprocessing.R) and keeping behaviours 1, 6, and 11. See table below for definitions. 

``` {r}
t.behav <- data.frame(table(birddat$BEH))
colnames(t.behav) <- c("behcode", "NumObservations")

lookup.behav <- read.csv("data/LOOKUP.BehaviourCodes.csv", header=T)
lookup.behav <- lookup.behav[order(lookup.behav$behcode),]

t.behav <- merge(lookup.behav, t.behav, all=T)
t.behav$NumObservations[is.na(t.behav$NumObservations)] <- 0
kable(t.behav, row.names=F)

birddat2 <- birddat[birddat$BEH %in% c("1", "6", "11"),]
```

### Add methodologies

Need to load method details from the projects summary and the definitions/descriptions of each duration and distance method code. 

``` {r}
projs <- read.csv("data/National_Proj_Summary_V4_2015.csv", header=T)
colnames(projs)[which(colnames(projs) == "Method")] <- "METHOD" #change colname
projs$Maxdist <- toupper(projs$Maxdist)

birddat3 <- merge(birddat2, projs[c("METHOD", "MaxDuration", "Maxdist")], by="METHOD", all.x = T)
kable(head(birddat3))
```

**NOTES**

* And here is where I realized that some of the new projects don't have max dist and max duration in their tables
* I referred back to the Access database to update the project summary table accordingly

``` {r}
meths <- read.csv("data/qry_ProjMethodSummary-manuallyCorrected.csv", header=T)

birddat3.5 <- merge(birddat3[c("METHOD", "PKEY", "SS", "SITE", "STN", "YYYY", "MM", "DD", "StartTime", "SPECIES", "English_Name", "BEH", "ABUND")], meths[c("METHOD", "DURMETH", "DURATIONRANGE", "DISTMETH", "DISTANCERANGE", "MaxDuration", "Maxdist")], by="METHOD", all.x=T)
```

#### Reclass methods to the subset I would use as factors in subsequent analyses

``` {r}
distanceClass <- data.frame(Maxdist=unique(birddat3.5$Maxdist), distanceClass=c("unlimited", "80-100", "80-100", "50"))

durationClass <- data.frame(MaxDuration=unique(birddat3.5$MaxDuration), durationClass=c("3", "5-6", "8-10", "8-10", "5-6"))

birddat3.5 <- merge(birddat3.5, distanceClass, by="Maxdist", all.x=T)
birddat3.5 <- merge(birddat3.5, durationClass, by="MaxDuration", all.x=T)
rm(birddat3)
```


### Aggregate to "total bird" count per survey instance


``` {r aggregate, eval=T}
sumABUND <- aggregate(birddat3.5$ABUND,  by=list(PKEY=birddat3.5$PKEY), FUN=sum)
colnames(sumABUND)[2] <- "sumABUND"

speciesCOUNT <- aggregate(birddat3.5$ABUND,  by=list(PKEY=birddat3.5$PKEY), FUN=length) 
colnames(speciesCOUNT)[2] <- "speciesCOUNT"

tmp <- merge(sumABUND, speciesCOUNT, by="PKEY")

birddat4 <- merge(tmp, birddat3.5[c("PKEY", "METHOD", "SS", "YYYY", "MaxDuration", "Maxdist", "durationClass", "distanceClass")], by="PKEY")

birddat4 <- birddat4[!duplicated(birddat4),]
```


### Combine with Peter's "total bird" offsets

**First, a quality check**

* How many of my kept PKEYs do not have total bird offsets? 
* Is it worth Peter recalculating offsets for them?

Ran this code previously to cache the most recent offset file I have
`offsets <- read.csv("data/offset-species-2016-12-13.csv", header=T)`

`cache("offsets")`

``` {r}
load("cache/offsets.RData")

colnames(offsets)
head(offsets$X)

offsetpkeys <-offsets$X
bcpkeys <- unique(birddat4$PKEY)

load("cache/TOTA_offsets-v3_2016-12-01.Rdata")
tota <- OFF; rm(OFF)
tota <- as.data.frame(tota)
tota$PKEY <- row.names(tota)
colnames(tota)
totapkeys <- tota$PKEY

```

##### Which PKEYs from BC are not in Peter's offsets? 

* `sum(bcpkeys %in% offsetpkeys)` `r sum(bcpkeys %in% offsetpkeys)` of `r length(bcpkeys)` BC Pkeys have offsets
* This suggests that `r length(bcpkeys) - sum(bcpkeys %in% offsetpkeys)` BC PKEYS do __not__ have offsets


##### What about the total species offsets? 
* `sum(bcpkeys %in% totapkeys)` `r sum(bcpkeys %in% totapkeys)` of `r length(bcpkeys)` BC Pkeys have offsets
* This suggests that `r length(bcpkeys) - sum(bcpkeys %in% totapkeys)` BC PKEYS do __not__ have offsets

``` {r}
df.offsettest <- data.frame(OFFSET.indivspp="yes", PKEY=offsets$X)
test <- merge(birddat4, df.offsettest, by="PKEY", all.x=T)
df.totatest <- data.frame(OFFSET.tota="yes", PKEY=totapkeys)
test <- merge(test, df.totatest, by="PKEY", all.x=T)

test$BothMissing <- (test$OFFSET.indivspp == "yes") & (test$OFFSET.tota == "yes")

nooffset <- test[is.na(test$BothMissing),]
write.table(nooffset, "output/BCdat_nooffset_2018.01.02.csv", sep=",", row.names = F, col.names = T)

kable(rbind(head(nooffset), tail(nooffset)),)

length(unique(nooffset$PKEY))
```

#### Re-calculate offsets since we're missing a bunch 

Code supplied by Peter Solymos on Jan 2, 2018. Note that this method uses only max distance and max duration, so it only corrects for methodology. Not location- or time-specific covariates. 

``` {r, eval=T}
require(QPAD)
x <- birddat4
str(x)
table(x$MaxDuration, useNA="a")
table(x$Maxdist, useNA="a")
levels(x$Maxdist)[levels(x$Maxdist) == "Unlimited"] <- "Inf"
x$MaxDistance <- as.numeric(as.character(x$Maxdist)) / 100
table(x$MaxDistance, useNA="a")

# numbers taken from estimates
phi_tota <- exp(0.2773876)
tau_tota <- exp(0.8422723)
x$p <- sra_fun(x$MaxDuration, phi_tota)
x$A <- ifelse(is.finite(x$MaxDistance), x$MaxDistance^2*pi, tau_tota^2*phi_tota)
x$q <- ifelse(is.finite(x$MaxDistance), edr_fun(x$MaxDistance, tau_tota), 1)
summary(x$p)
summary(x$A)
summary(x$q)

x$Correction <- with(x, p * A * q)
x$Offset <- log(x$Correction)
summary(x$Offset)
length(x$Offset)
```


#### Combine bird data with offsets

**If applicable, recombine the calculated offsets with Peter's previously calculated offsets**

As of Jan 3, 2017. I re-calculated all offsets using the simplified approach, so this chunk isn't necessary.

``` {r eval=F}
row.names(x) <- x$PKEY
colnames(x)[which(colnames(x) == "Offset")] <- "TOTA"
x <- x[c("TOTA", "PKEY")]
tota2 <- rbind(tota, x)
sum(duplicated(tota2$PKEY)) #none are duplicated!

birddat5 <- merge(birddat4, tota2, by="PKEY", all.x=T)
sum(is.na(birddat5$TOTA)) # how many PKEYs don't have offsets
```

**If re-calculating the offsets for all PKEYs, use the following chunk**

``` {r}
birddat5 <- x
sum(is.na(birddat5$Offset)) # how many PKEYs don't have offsets
```

### Combine bird data with VRI data

``` {r}
bird_vri_dat_ready <- merge(birddat5, vri_bird_tempaligned[-which(colnames(vri_bird_tempaligned) %in% c("FID_BIRD", "VRIintersectLayer", "OBJECTID", "FID_VRI", "BirdLayer", "MAP_ID", "POLYGON_ID", "OPENING_IND", "OPENING_SOURCE", "POLYGON_AREA", "INPUT_DATE", "INTERPRETATION_DATE", "REFERENCE_YEAR", "ATTRIBUTION_BASE_DATE", "PROJECTED_DATE", "OPENING_ID", "ORG_UNIT_NO", "ORG_UNIT_CODE", "FD_PERCENT", "PL_PERCENT", "AT_PERCENT", "VRI.layer.polygon.ID",  "ToleratedLag", "LagExceedsTolerated", "VRI.layer.polygon.ID.PKEY", "YYYY", "SS"))], by="PKEY", all.x=T)
bird_vri_dat_ready$ForID <- with(bird_vri_dat_ready, paste(BecZone, SpeciesGroup, Age, Height, sep="."))
bird_vri_dat_ready <- bird_vri_dat_ready[sort(colnames(bird_vri_dat_ready))]

kable(bird_vri_dat_ready[sample(nrow(bird_vri_dat_ready), 10),c("ForID", "BecZone", "SpeciesGroup", "Age", "Height", "AgeClass_calc", "HeightClass_calc", "DisturbanceYear", "speciesCOUNT", "sumABUND")])
```

### Check if offsets are the same within a given survey method

``` {r}
methods <- unique(bird_vri_dat_ready$METHOD)

## function to look at unique offsets within a given method. Should have one value per row/method.
kable(do.call(rbind, lapply(methods, function(x) {
  unique(bird_vri_dat_ready[bird_vri_dat_ready$METHOD == x,]$Offset)
})))

```

``` {r}
cache("bird_vri_dat_ready")
```