---
title: "00.01.MergeAvianFiles"
author: "Nicole Barker"
date: "Last run: Jan 4, 2017"
output: 
  word_document:
    reference_docx: ../styles/ReportFormat_1.docx
---
## Script Abstract

Quality-checks, corrects, pre-processes, and merges the various tables from BAM's Avian Database. Removes duplicates. Performs some initial tests of patterns in avian data by survey method to help decide how to harmonize the data. 
Combines spatial data (SS), survey instance (PKEY), and point count (ABUND) data into one flat dataset and saves as csv for future use: _data/birddata_preprocess1.csv_


## Background
On Nov 30, 2017, Trish provided me with the Access Database of BAM's avian data: COFI_BC_NOV30_2017.accdb. I exported the 3 tables based on BAM's standard data format.

**FILES**

1. **BC_COFI_XY.csv**
2. **BC_COFI_PKEY.txt**
3. **BC_COFI_POINTCOUNT.txt**

This script does the following

* Look for and eliminate duplications
* Correct any errors noticed during pre-processing
* Save a pre-processed table of point count data for future use: data/birddata_preprocess1.csv 
* 


``` {r setup, echo=F, message=F, warning=F}
require(knitr)
opts_knit$set(root.dir = '..')
```

``` {r load.project, message=F}
require(ProjectTemplate)
load.project()
options(digits=12)
```

### Initial Quality Check and Removal of Duplicates
#### 1. XY Coordinates of each survey site: *BC_COFI_XY.txt*
``` {r load.xy}
xy<- read.csv("data/BC_COFI_XY.csv")
kable(head(xy), row.names=F)
```

**Checking for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind,lapply(xy, function(x) {sum(is.na(x))}))))
```

##### Remove any SS without x and y coordinates
``` {r remove.no.coordinates, eval=T}
xy.qs <- xy
xy <- xy[!is.na(xy$X),]
xy <- xy[xy$X != 0,]
```

**Checking for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind,lapply(xy, function(x) {sum(is.na(x))}))))

```




**Notes**

* `r unique(xy.qs[is.na(xy.qs$X),]$PCODE)` have SSs without X coordinates
* ACTION: Suggest Trish could look into datasets without coordinates. 

#### 2. Sampling Occasions: *BC_COFI_PKEY.txt*
It's useful to look at a map for this. Kathy Martin's data represents a good example. 

* _PCODE_: unique code for each project
* _SITE_: Typically a cluster of point count stations
* _STN_: individual point count survey location
* _SS_: compound key comprised of PCODE:SITE:STN
* _ROUND_: If multiple visits to the same location, typically on different days.
* _PKEY_: compound key -->  PCODE:SITE:STN:YY:ROUND
* _METHOD_: The survey method (survey distance, duration); usually but not always consistent within a PCODE.
* _obs_: Identity of the survey observer.

![ ^^^ Image. BBS, Atlas (BCCA), and KMART (Kathy Martin)'s data, as an example of PCODE, SITE, and STN. Different coloured dots are from different projects (PCODEs). Kathy Martin's data (KMART) has clusters of stations in different sites (SITE), which are labelled KNIFEAFF, KNIFE7M, etc. Within (SITE) clusters are individual stations (STN). The combination of PCODE:SITE:STN makes up SS, which is a  unique ID corresponding to a given location indicated by xy coordinates](../output/KathyMartinSITEdemo.jpg)

``` {r load.pkey}
pkey <- read.csv("data/BC_COFI_PKEY.txt")
kable(rbind(head(pkey), tail(pkey)), row.names=F)
unique(pkey$METHOD)
```

**Checking for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind, lapply(pkey, function(x) {sum(is.na(x))}))))
```

``` {r merge.xy.pkey, eval=T}
length(unique(xy$SS))
length(unique(xy$PCODE))
xy.pkey <- merge(xy[c("PCODE", "SS", "X", "Y")], 
               pkey[c("PCODE", "SS", "METHOD", "SITE", "STN", "ROUND", "PKEY", "YYYY", "MM", "DD", "StartTime", "obs")], 
               by="SS",
               all=T)
xy.pkey$PCODE_derived <- unlist(lapply(strsplit(as.character(xy.pkey$SS), ":"), function(x) {x[1]}))
xy.pkey$PCODE.SITE <- paste(xy.pkey$PCODE_derived, xy.pkey$SITE, sep=".")
```

**Checking for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind, lapply(xy.pkey, function(x) {sum(is.na(x))}))))
```

##### Fix Dates

* remove sites without dates. The following PCODEs all have at least one SS without a MM: `r unique(xy.pkey[is.na(xy.pkey$MM),]$PCODE)` --> I since decided to keep in those without MM, for now. 
* Switch MM and DD for GMSMON15

``` {r fix.dates, eval=T}
xy.pkey.qs <- xy.pkey
#xy.pkey <- xy.pkey[!is.na(xy.pkey$MM),]
#xy.pkey <- xy.pkey[xy.pkey$MM != 0,]

unique(xy.pkey$MM) #just checking for NA or 0 values
```

**ERROR NOTICED** - I'm pretty sure no surveys were done in Nov and Dec, so this probably indicates a switch in MM and DD for some sites. Time to track down which ones 

``` {r}
xy.pkey$MM.old <- xy.pkey$MM
xy.pkey$DD.old <- xy.pkey$DD
xy.pkey <- xy.pkey[order(xy.pkey$YYYY, xy.pkey$SS),]

kable(rbind(head(xy.pkey[xy.pkey$PCODE_derived %in% "GMSMON15", c("PCODE_derived", "SS", "YYYY", "MM", "DD", "MM.old")], 10), tail(xy.pkey[xy.pkey$PCODE_derived %in% "GMSMON15", c("PCODE_derived", "SS", "YYYY", "MM", "DD", "MM.old")], 10)), row.names=F)

xy.pkey$DD[xy.pkey$PCODE_derived %in% "GMSMON15" & xy.pkey$YYYY == "2012"] <- xy.pkey$MM.old[xy.pkey$PCODE_derived %in% "GMSMON15" & xy.pkey$YYYY == "2012"] 

xy.pkey$MM[xy.pkey$PCODE_derived %in% "GMSMON15" & xy.pkey$YYYY == "2012"] <-
  xy.pkey$DD.old[xy.pkey$PCODE_derived %in% "GMSMON15" & xy.pkey$YYYY == "2012"]

kable(rbind(head(xy.pkey[xy.pkey$PCODE_derived %in% "GMSMON15" & xy.pkey$YYYY == "2012",c("PCODE_derived", "SS", "YYYY", "MM", "DD")]),
            tail(xy.pkey[xy.pkey$PCODE_derived %in% "GMSMON15" & xy.pkey$YYYY == "2012",c("PCODE_derived", "SS", "YYYY", "MM", "DD")])), row.names=F)
unique(xy.pkey$MM)
```

Alright, so that fixed the obvious date issues. Let's look at some other potential data problems. 

``` {r}
xy.pkey$DATE <- as.Date(paste(xy.pkey$YYYY, xy.pkey$MM, xy.pkey$DD, sep="/"))
```

**Notes**

* The XY.PKEY table has `r nrow(xy.pkey)` rows covering `r length(unique(xy.pkey$SS))` SS from `r length(unique(xy.pkey$SITE))` sites over `r length(unique(xy.pkey$PCODE))` projects. 
* The earliest point count was done in `r min(xy.pkey$YYYY)`.
* Sometimes the addition of WSI data added projects we already had from the Atlas. So we need to look for duplicated locations and years to remove those duplicates.

##### Looking for Duplicates 1: initial exploration of duplicates by Location and Date

``` {r, create.locYr}
xy.pkey$LocYr <- paste(xy.pkey$X, xy.pkey$Y, xy.pkey$YYYY)
kable(head(xy.pkey[c("PCODE.SITE", "LocYr", "DATE", "StartTime")]), row.names=F)
```

`r length(unique(xy.pkey$LocYr))` unique location & year combinations but in a data.farme of `r nrow(xy.pkey)` rows. Indicating `r nrow(xy.pkey) -length(unique(xy.pkey$LocYr))` duplicated combinations of location and year.

Let's look at some of these duplicates in more detail. 

``` {r loc.yr.duplicates, eval=T}
loc.yr.dups <- xy.pkey$LocYr[duplicated(xy.pkey$LocYr)] # which combos are duplicated?
xy.pkey.dups <- xy.pkey[xy.pkey$LocYr %in% loc.yr.dups,] #subset for duplicated combos
xy.pkey.dups <- xy.pkey.dups[order(xy.pkey.dups$LocYr),] #change order
write.table(xy.pkey.dups, file="output/duplicates.xyYear.csv", sep=",", col.names=T, row.names=F) #archive to computer
kable(xy.pkey.dups[1:20,c("LocYr", "PCODE.SITE", "STN", "StartTime", "obs")], row.names=F) #preview
```

Thoughts on the above table: 

* Some "duplicate" surveys start at different times and correspond to different STNs. This suggests that they are different stations, even though they have "identical" xy coordinates. Perhaps the precision on the XY coordinates isn't sufficient to distinguish separate points. 

![ ^^^ Image. Example for SS BCCA:11PQ75:310765 and BCCA:11PQ75:310765, where bird data are different, confirming two different surveys... though possibly unindicated rounds](../output/ExampleDataDuplication2.jpg)

* Subsequent examination of a specific known duplicate set (Atlas BCCA and QDFA) confirms that coordinates for some datasets have been rounded to fewer decimal places than were originally included. This would have the opposite effect of appearing to be NOT duplicated when in reality they are.
    * Short-term ACTION: Round XY coordinates to 6 digits so that I detect duplication between less and more precise datasets.
    * Longer-term ACTION: Ask Trish if Atlas has more precise coordinates stashed somewhere.

![ ^^^ Image. Example for of the exact same stations being included from two different sources (BCCA and QDFA). Survey information is identical other than XY. BCCA has lower precision on XY coordinates than does QDFA.](../output/ExampleDataDuplication3.jpg)

* In conclusion, the current LocYr combination is not a sufficient indicator for identifying duplicates
    * Short-term ACTION: Include date, start time, and observer in my calculation of duplicates
    * Longer-term ACTION: Trish can inspect apparent duplicates and delete real duplications. 

##### Removing Duplicates 2: Adding Time and Observer into the calculation

``` {r Loc.Date.Time.Duplicates, eval=T}
xy.pkey$LocDateTime <- paste(round(xy.pkey$X, 6), round(xy.pkey$Y,6), xy.pkey$DATE, xy.pkey$StartTime)
xy.pkey$LocDateTimeObs <- paste(round(xy.pkey$X, 6), round(xy.pkey$Y,6), xy.pkey$DATE, xy.pkey$StartTime, xy.pkey$obs)
xy.pkey <- xy.pkey[order(xy.pkey$LocDateTimeObs),]
```

**Notes** 

* `r length(unique(xy.pkey$LocDateTime))` unique location, date, and time combinations but in a data.farme of `r nrow(xy.pkey)` rows. Indicating many fewer, but still `r nrow(xy.pkey)- length(unique(xy.pkey$LocDateTime))`, duplicates. 
* But... there's a difference between those containing StartTime and those containing StartTime AND observer: `r length(unique(xy.pkey$LocDateTimeObs))` unique combinations of location, date, time, and observer, `r length(unique(xy.pkey$LocDateTimeObs))- length(unique(xy.pkey$LocDateTime))` more than just combining location, date, and time.
    * ACTION: Before I delete them, I should check if they're genuine double-observer surveys.

##### Removing Duplicates 3: Looking for double-observer surveys

Look for duplicated combinations of locdatetime that are NOT in the locdatetimeobs table

``` {r look.for.double.observers, eval=T}
locdatetimedups <- xy.pkey$LocDateTime[duplicated(xy.pkey$LocDateTime)] #duplicated combos
locdatetimeobsdups <- xy.pkey$LocDateTimeObs[duplicated(xy.pkey$LocDateTimeObs)] # duplicated combos
xy.pkey.locdatetimedups <- xy.pkey[xy.pkey$LocDateTime %in% locdatetimedups,] #df of duplicated combos
xy.pkey.locdatetimeobsdups <- xy.pkey[xy.pkey$LocDateTimeObs %in% locdatetimeobsdups,] #df of duplicated combos

possible.double.observers <- xy.pkey.locdatetimedups[!xy.pkey.locdatetimedups$LocDateTime %in% xy.pkey.locdatetimeobsdups$LocDateTime, c("PCODE.SITE", "STN", "ROUND", "DATE", "StartTime", "obs", "LocDateTimeObs")] #subset of combos where observer is only difference between apparently duplicated sites
possible.double.observers <- possible.double.observers[order(possible.double.observers$PCODE.SITE, possible.double.observers$DATE, possible.double.observers$StartTime),] #change order
write.table(possible.double.observers, file="output/duplicates.possibledoubleobservers.csv", sep=",", col.names=T, row.names=F) #archive on computer
kable(possible.double.observers[c(1:10, 20:25, 200:210, 230), c("PCODE.SITE", "STN", "ROUND", "DATE", "StartTime", "obs")]) #preview
```

**Notes**

It's a small set, so I inspected them all manually in Excel and in Access. I came to the following conclusions: 

* Only one survey appears to be true double-observer surveys: KMART:RISKESD has observers LMT and KD, who observed similar but not identical bird lists. ACTION: Keep these ones. 
  
* Some appear to be duplicated datasets... giving the impression of two different observers because they're numbers in BCCA but names in the original project. e.g., 
    * BCCA10FG08 and BL2TFL48 in 2008. Atlas observer 970 is probably Kelly Squire. 
    * BCCA and QDFA in 2008 and 2009. 
        * Atlas observer 82 = Chris Chutter; 
        * Observer 1263 = Christine Rothenbach; 
        * Observer 467 = James Bradley; 
        * Observer 1367 = Kate England. 
    * ACTION: Delete the ATLAS versions of these but keep the project specific ones. 

* And still others are inexplicably weird. Pairs of apparently different observers conducting a survey at the same location and time, but with very different species lists. 
    * BCCA:10DU79 and BCCA:10DU89 appear to have two observers: one unidentified and one 99 or 100. Inspecting the Access Database suggests that this is NOT double-observer (see below screencap). I wonder if somehow the same observer collected all the data at this station, but didn't fill in all the rows with the observer ID. This led to splitting the survey into two parts? ACTION: DELETE these sites

![ ^^^ Image. Example BCCA:10DU79 and BCCA:10DU89 on Jun 5, 2009 and Jun 17, 2009](../output/ExampleDataDuplication4.jpg)

  * BCCA:10FE54 has observers 120 and 965. Similar situation as above where observer  120 has a much bigger species list than does observer 965. ACTION: Delete observer 965 but keep 120. 

![ ^^^ Image. Example BCCA:10FE54](../output/ExampleDataDuplication5.jpg)

**ACTIONS**

* I created an Excel file summarizing the verdict on this specific subset of duplicates: ** duplicates.possibledoubleobservers-deleteVerdict.csv.** Use this to remove undesired sites/surveys. 

``` {r remove.unwanted.set1, eval=T}
removeverdict <- read.csv("output/duplicates.possibledoubleobservers-deleteVerdict.csv", header=T)
LocDateTimeObs.todelete <- unique(removeverdict$LocDateTimeObs) #which combos to delete
xy.pkey.qs <- xy.pkey #create quick.save version of df.
xy.pkey <- xy.pkey[!xy.pkey$LocDateTimeObs %in% LocDateTimeObs.todelete,] #subsetting for not the bad combos
```

##### Removing Duplicates 4: Returning to regular duplicates (LocationDateTimeObserver)

Now that I've taken care of surveys that _appeared_ to be double-observer (b/c different observers for all other identical survey info), I can focus on examining the duplicates for Location, Date, Time, and Observer. 

``` {r}
write.table(xy.pkey.locdatetimeobsdups, file="output/duplicates.LocYearTimeObs.csv", sep=",", col.names=T, row.names=F) #archive to computer
xy.pkey.locdatetimeobsdups <- xy.pkey.locdatetimeobsdups[order(xy.pkey.locdatetimeobsdups$PCODE.SITE, xy.pkey.locdatetimeobsdups$YYYY, xy.pkey.locdatetimeobsdups$MM, xy.pkey.locdatetimeobsdups$DD),]

kable(rbind(head(xy.pkey.locdatetimeobsdups[c("PCODE.SITE", "STN", "ROUND", "DATE", "StartTime", "obs")], 10), tail(xy.pkey.locdatetimeobsdups[c("PCODE.SITE", "STN", "ROUND", "DATE", "StartTime", "obs")], 10)))
kable(xy.pkey.locdatetimeobsdups[c(1:25, (nrow(xy.pkey.locdatetimeobsdups)-25):nrow(xy.pkey.locdatetimeobsdups)), c("PCODE.SITE", "STN", "ROUND", "DATE", "StartTime", "obs")], row.names=F, caption="First 25 rows and Last 25 rows")
```

**Notes**

Investigating on a case by case basis. 

* SRDR.M68_3: There's a STN 8 and a STN 9 at the exact same coordinates, surveyed at the exact same time. But with different species lists. ACTION: Keep both for now. 
* KMART:KNIFEKN: B2: looks like it might be a typo extra space in the SS/STN. Results in different bird data, but I think maybe it's supposed to be for the same site.       
    * ACTION: I changed the SS and STN name in the hopes that it would fix the problem, but it probably won't. I may need to remove this STN from the dataset. 
* BCCA: In some of the pairs I checked, the surveys are at the same location, started at the same time, and done by the same observer. They have similar but not identical bird list.

![ ^^^ Image. Example BCCA:08MM11 at 6:37 am by observer 117. Entry 1 has FOSP while Entry 2 doesn't; Entry 2 has SAVS and WCSP while Entry 1 doesn't](../output/ExampleDataDuplication6.jpg)

* But in other pairs, everything is identical (location, date, time, observer, AND bird list). I suggest there are errors somewhere, but I can't figure out where.

![ ^^^ Image. Example for SS BCCA:11NS17:333027 and BCCA:11NS17:333029, suggesting complete duplication of data](../output/ExampleDataDuplication1.jpg)

![ ^^^ Image. Example for BCCA:10CE80, suggesting complete duplication of data](../output/ExampleDataDuplication7.jpg)

* Short-term ACTION: Delete these from the dataset in the interest of saving time. 
* Long-term ACTION: Take this list to Trish for more exploration. 


``` {r delete.latest.duplicates, eval=T}
xy.pkey.qs <- xy.pkey #quicksave
xy.pkey <- xy.pkey[!xy.pkey$LocDateTimeObs %in% xy.pkey.locdatetimeobsdups$LocDateTimeObs,]
```

After that, I hope there aren't any duplicates left. 


#### 3. Point Count Data (i.e., bird observations): *BC_COFI_POINTCOUNT.txt* 

**FILES**

1. **BC_COFI_POINTCOUNT.csv**

``` {r load.pcdat, eval=T}
#load point count data
pcdat1 <- read.csv("data/BC_COFI_POINTCOUNT.txt")
colnames(pcdat1)[which(colnames(pcdat1)=="SumOfABUND")] <- "ABUND"
pcdat1 <- pcdat1[c("PKEY", "DURATION", "DISTANCE", "SPECIES", "BEH", "ABUND")]

# load species codes/names
#codes <- read.csv("data/EC_AVIAN_CORE_20131015.csv") # OLDER species list
codes <- read.csv("data/EC_AVIAN_CORE_20150324.csv")
colnames(codes)[which(colnames(codes)=="Species_ID")] <- "SPECIES"

#merge species names into point count dataset
pcdat2 <- merge(pcdat1, codes, by="SPECIES", all.x=T)

#subset for necessary columns
pcdat <- pcdat2[c(colnames(pcdat1), "English_Name")]
pcdat <- pcdat[order(pcdat$PKEY),]

kable(rbind(head(pcdat), tail(pcdat)), row.names=F, caption="First 6 and last 6 rows")
```

##### Looking for duplicated bird observations


``` {r melt.pc.dat, eval=T}
pcdat_melt <- melt(pcdat[c("PKEY", "SPECIES", "DURATION", "DISTANCE", "ABUND", "BEH")], measure.vars="ABUND") # bring dataset into molten form
```

Let's look for duplicates here... Combine PKEY, Species, Duration, Distance, and Behaviour to create a unique ID.
 
``` {r look.for.duplicates}
pcdat_melt$PkeySpDurDisBeh <- paste(pcdat_melt$PKEY, pcdat_melt$SPECIES, pcdat_melt$DURATION, pcdat_melt$DISTANCE, pcdat_melt$BEH, sep=".")
```

`r length(unique(pcdat_melt$PkeySpDurDisBeh))` unique combinations of PKEY, SPECIES, DURATION, DISTANCE, AND BEHAVIOUR but `r nrow(pcdat_melt)` rows. suggesting `r nrow(pcdat_melt) - length(unique(pcdat_melt$PkeySpDurDisBeh))` duplicates

NO DUPLICATES. 

``` {r finding.UNGU.related.duplicates, eval=F}
dupcombos <- unique(pcdat_melt[duplicated(pcdat_melt$PkeySpDurDisBeh),]$PkeySpDurDisBeh)
kable(data.frame(PkeySpDurDistBeh=dupcombos))
pcdat_melt_tmp <- pcdat_melt[pcdat_melt$PkeySpDurDisBeh %in% dupcombos,]
kable(pcdat_melt_tmp, row.names=F)
kable(head(pcdat[pcdat$SPECIES %in% "UNGU",]), row.names=F)
codes[duplicated(codes$SPECIES),] #finding that duplicated species code...
# All of these duplicates involve the same Species code (UNGU) -- the duplicates are from when I merged in the English names!! So I went back and updated my species code list to the 2015 version and now there's no duplicates. 
```

#### Combine PC dat with xy.pkey and write to a file for future use

##### First, check which PKEY have no bird data, and which bird data have no associated spatial data

``` {r}
xy.pkey.pc1 <- merge(xy.pkey[c("SITE", "STN", "METHOD", "YYYY", "MM", "DD", "ROUND", "StartTime", "PKEY")], pcdat, by="PKEY", all.x =T) # merge with all pkey in the xy or pkey table, regardless of presence of bird data

kable(rbind(head(xy.pkey.pc1[is.na(xy.pkey.pc1$ABUND),]),
            tail(xy.pkey.pc1[is.na(xy.pkey.pc1$ABUND),])))

``` 

**Check for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind,lapply(xy.pkey.pc1, function(x) {sum(is.na(x))})))) # check for missing data
```

`r length(xy.pkey.pc1[is.na(xy.pkey.pc1$ABUND),]$PKEY)` unique PKEYs have no bird data associated with them. In PCODEs: `r unique(xy.pkey.pc1[is.na(xy.pkey.pc1$ABUND),]$PCODE)`

``` {r}
xy.pkey.pc2 <- merge(xy.pkey[c("SITE", "STN", "METHOD", "YYYY", "MM", "DD", "ROUND", "StartTime", "PKEY")], pcdat, by="PKEY", all.y =T) # merge with all abund data, regardless of presence in xy or pkey tables

kable(rbind(head(xy.pkey.pc2[is.na(xy.pkey.pc2$PCODE_derived),]),
            tail(xy.pkey.pc2[is.na(xy.pkey.pc2$PCODE_derived),])))

``` 

**Check for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind, lapply(xy.pkey.pc2, function(x) {sum(is.na(x))})))) # check for missing data
```

`r length(unique(xy.pkey.pc2[is.na(xy.pkey.pc2$PCODE),]$PKEY))` unique PKEYs have bird data, but are missing some element from survey method such as spatial location, date, etc. In PCODEs: `r unique(xy.pkey.pc2[is.na(xy.pkey.pc2$PCODE),]$PCODE_derived)`

``` {r}
xy.pkey.pc3 <- merge(xy.pkey[c("SITE", "STN", "METHOD", "YYYY", "MM", "DD", "ROUND", "StartTime", "PKEY")], pcdat, by="PKEY", all =T) # keeps all pkeys even if missing some data

``` 

**Check for Missing Data**

``` {r}

kable(as.data.frame(do.call(rbind,lapply(xy.pkey.pc3, function(x) {sum(is.na(x))})))) # check for missing data
```

``` {r}
xy.pkey.pc4 <- merge(xy.pkey[c("SITE", "STN", "METHOD", "YYYY", "MM", "DD", "ROUND", "StartTime", "PKEY")], pcdat, by="PKEY") # merge only where we have all data

head(xy.pkey.pc4[is.na(xy.pkey.pc4$ABUND),])

``` 

**Check for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind,lapply(xy.pkey.pc4, function(x) {sum(is.na(x))})))) # check for missing data
```

In omitting PKEYs where we're missing some data (either bird data or survey / methodological information), we omit `r length(unique(xy.pkey.pc3$PKEY)) - length(unique(xy.pkey.pc4$PKEY))` PKEYs. 

##### Trying to understand why some of my PKEYs are missing METHOD

``` {r}
missingmethod <- xy.pkey.pc3[is.na(xy.pkey.pc3$METHOD),c("PKEY", "YYYY", "MM", "DD")]
missingmethod <- missingmethod[!duplicated(missingmethod),]
missingmethod$HaveBirdDat <- "Yes"
head(missingmethod)

# Test with first one

"BCCA:08PL21:341190:12:1" %in% xy.pkey$PKEY # this PKEY with a missing method is not in the xy.pkey table
"BCCA:08PL21:341190:12:1" %in% pcdat$PKEY   # this PKEY with a missing method is in the pcdat table

sum(missingmethod$PKEY %in% xy.pkey$PKEY) # how many PKEYs with missing methods are in the PKEY table?
sum(missingmethod$PKEY %in% pcdat$PKEY) # how many PKEYs with missing methods are in the PCDAT table?

pcdattab.pkeys <- data.frame(PKEY = unique(pcdat$PKEY), inPCDATtable="yes")  
nrow(pcdattab.pkeys)

pkeytab.pkeys <- data.frame(PKEY=unique(xy.pkey$PKEY), inPKEYtable="yes")
nrow(pkeytab.pkeys)

pkeys <- merge(pkeytab.pkeys, pcdattab.pkeys, by="PKEY", all=T)
pkeys$MissingSomewhere <- pkeys$inPKEYtable != pkeys$inPCDATtable
pkeys$MissingSomewhere[is.na(pkeys$MissingSomewhere)] <- "Yes"
pkeys <- pkeys[order(pkeys$Missing),]
head(pkeys)
tail(pkeys)

missingsomewhere <- pkeys[pkeys$MissingSomewhere == "Yes",]
write.table(missingsomewhere, "output/MissingPKEYs.csv", col.names = T, row.names=F, sep=",")
```

### Add a bit more info to the tables

##### Derive PCODE and SS where missing

``` {r}
xy.pkey.pc.use <- xy.pkey.pc3
xy.pkey.pc.use$PCODE_derived <- unlist(lapply(strsplit(as.character(xy.pkey.pc.use$PKEY), ":"), function(x) {x[1]}))
xy.pkey.pc.use$SS_derived <- unlist(lapply(strsplit(as.character(xy.pkey.pc.use$PKEY), ":"), function(x) {paste(x[1:3], collapse=":")}))
```

**Check for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind,lapply(xy.pkey.pc.use, function(x) {sum(is.na(x))})))) # check for missing data
```


##### Add Methods

**Get METHOD from PKEY Table**

* Noting that there are some PKEYs in the pcdat table that don't have a method

``` {r}
pkey <- read.csv("data/BC_COFI_PKEY.txt")
pkey.meth <- pkey[c("PKEY", "METHOD")]
pkey.meth <- pkey.meth[!duplicated(pkey.meth),]
``` 

**Get method details (MaxDur, MaxDist, etc) from proj summary table and method code tables** 

1. **National_Proj_Summary_V4_2015.csv**

```{r, load.proj.summary}
projs <- read.csv("data/National_Proj_Summary_V4_2015.csv", header=T)
colnames(projs)[which(colnames(projs) == "Method")] <- "METHOD" #change colname
projs.qs <- projs #quicksave
projs$Maxdist <- toupper(projs$Maxdist)

kable(head(projs[c("PCODE", "METHOD", "DURMETH", "DISTMETH", "ChangeMethod", "MaxDuration", "Maxdist")]), row.names=F)
```

**Add method details to pkey table**

``` {r}
pkey.meth.details <- merge(pkey.meth, projs, by="METHOD", all.x=T)
colnames(pkey.meth.details)
pkey.meth.details <- pkey.meth.details[c("PKEY", "METHOD", "DURMETH", "DISTMETH", "MaxDuration", "Maxdist")] 
kable(head(pkey.meth.details))
```

**Check for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind,lapply(pkey.meth.details, function(x) {sum(is.na(x))})))) # check for missing data
```

**NOTES**

* Some of the new projects don't have max dist and max duration in their tables
* I referred back to the Access database to update the project summary table accordingly

``` {r}
projs <- read.csv("data/qry_ProjMethodSummary-manuallyCorrected.csv", header=T)

colnames(projs)[which(colnames(projs) == "Method")] <- "METHOD" #change colname
projs.qs <- projs #quicksave
projs$Maxdist <- toupper(projs$Maxdist)

kable(head(projs[c("PCODE", "METHOD", "DURMETH", "DISTMETH", "MaxDuration", "Maxdist")]), row.names=F)
```

``` {r}
rm(pkey.meth.details)
pkey.meth.details <- merge(pkey.meth, projs, by="METHOD", all.x=T)
colnames(pkey.meth.details)
pkey.meth.details <- pkey.meth.details[c("PKEY", "METHOD", "DURMETH", "DISTMETH", "MaxDuration", "Maxdist")] 
```

**Merge methods back into the xy.pkey.pcdat table**

``` {r}
xy.pkey.pc.use2 <- merge(xy.pkey.pc.use[-which(colnames(xy.pkey.pc.use) == "METHOD")], pkey.meth.details, by="PKEY", all.x=T)
colnames(xy.pkey.pc.use2)
head(xy.pkey.pc.use2)
```

**Check for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind,lapply(xy.pkey.pc.use2, function(x) {sum(is.na(x))})))) # check for missing data
```

**NOTES**

* Still missing some data but I don't have time to nit-pick right now

##### Write the table for later

``` {r}
write.table(xy.pkey.pc.use2, file="data/birddata_preprocess1.csv", sep=",", row.names=F, col.names=T)
```


## Check for Systematic Differences Among Surveys

BC Models are to be built for all species, not individual species. Peter explored the possibility of a Total Bird offset, which corrects for survey methodlogy and detectability in general, but not individual species detectability. When my preliminary results didn't follow expected patterns in bird response to forest age and height, we started to wonder if the total offset is obscuring some of those patterns. 

Peter suggested scaling all point counts to the same time/distance and then running analyses without a correction factor offset. This will be challenging if I want to include BBS data, which are unlimited distance surveys. 

First thing to check... whether species means differ by survey method. 

#### Which surveys use the same methods?

* Survey method is indicated at the PKEY level
    * Merge PKEY table with point count data to link method to data from individual point counts
    * Merge resultant table with method legend so I know the distance and duration methods used. 
    * Calculate means and counts of surveys per method
    
If there are systematic differences in mean abundance, I can't simiply lump all data together in one analysis because variation caused by methods will cause non-random spatial biases in abundances. 

``` {r merged.xy.pkey.pc, eval=T}
xy.pkey.pc <- xy.pkey.pc.use2
colnames(xy.pkey.pc)[which(colnames(xy.pkey.pc) == "PCODE_derived")] <- "PCODE"
```

Filter for those that don't have method

```{r}
xy.pkey.pc <- xy.pkey.pc[!is.na(xy.pkey.pc$METHOD),]
```


Create unique ID for combination of duration and distance method

``` {r}
xy.pkey.pc$DurDisKey <- paste(xy.pkey.pc$DURMETH, xy.pkey.pc$DISTMETH, sep=".")
```

#### Compare bird counts among different survey types

`r length(unique(xy.pkey.pc$DurDisKey))` different combinations of methods used, across `r length(unique(xy.pkey.pc$PCODE))` different projects.

Subset to round1 and behaviour = 6 only

``` {r}
xy.pkey.pc.round1 <- xy.pkey.pc[xy.pkey.pc$ROUND == 1 & xy.pkey.pc$BEH == 6,]
```

Aggregate to "total bird" count per survey instance

``` {r aggregate, eval=T}
pkey.sumabund <- aggregate(xy.pkey.pc.round1$ABUND,  by=list(PKEY=xy.pkey.pc.round1$PKEY), FUN=sum)
colnames(pkey.sumabund)[2] <- "sumABUND"
```

Merge with original dataset to get details of methods

``` {r reattribute.method}
pkey.method <- xy.pkey.pc.round1[c("PKEY", "DISTMETH", "DURMETH", "METHOD", "MaxDuration", "Maxdist", "DurDisKey")]
pkey.method <- pkey.method[!duplicated(pkey.method),]

pkey.sumabund <- merge(pkey.sumabund, pkey.method, by="PKEY")
```

Calculate mean abundance by method

``` {r compare.bird.means.by.method, eval=T}
method.means <- aggregate(pkey.sumabund$sumABUND, by=list(DurDisKey=pkey.sumabund$DurDisKey), FUN=mean) # abundance per unique method
colnames(method.means)[2] <- "MeanAbund"

method.counts <- aggregate(pkey.sumabund$sumABUND, by=list(DurDisKey=pkey.sumabund$DurDisKey), FUN=length) # number of PKEYs (i.e. survey occasions) per unique method. 
colnames(method.counts)[2] <- "CountSurveys"

# Create a key for all methods represented in the BC dataset
durcodes <- read.csv("data/DD_duration_codes_methodology.csv", header=T)
discodes <- read.csv("data/DD_distance_codes_methodology.csv", header=T)
colnames(durcodes)[2] <- "DURMETH"
colnames(discodes)[1] <- "DISTMETH"
method.lookup <- pkey.method[c("DISTMETH", "DURMETH", "DurDisKey", "MaxDuration", "Maxdist")]
method.lookup <- method.lookup[!duplicated(method.lookup),]
method.lookup <- merge(method.lookup, durcodes, by="DURMETH", all.x=T)
method.lookup <- merge(method.lookup, discodes, by="DISTMETH", all.x=T)

method.means <- merge(method.means, method.lookup[c("DurDisKey", "DISTMETH", "DURMETH", "MaxDuration", "Maxdist", "DURATIONRANGE", "DISTANCERANGE")], by="DurDisKey")
method.stats <- merge(method.means, method.counts, by="DurDisKey")

method.stats <- method.stats[order(method.stats$MaxDuration, method.stats$Maxdist), c("DurDisKey", "DURMETH","DISTMETH", "MaxDuration", "Maxdist", "DURATIONRANGE", "DISTANCERANGE", "CountSurveys", "MeanAbund")]

kable(method.stats, row.names=F)
```

## Conclusions

* Yup there are systematic differences among surveys / methods, so I can't simply lump everything in a single analysis. 
* I met with Peter Solymos on Dec 11 to discuss, and we came up with the following plans: 
    * Peter thinks his total bird offset still makes sense. It corrects for survey method and for effects of time of year and of day on detectability... just not for species-specific EDR and singing rate, for example. 
    * If I want an alternative I could do the following: 
        * Use survey method as correction FACTORs in model, based on max duration and max distance
            * 3 levels for duration: max duration is 3, 5/6, or 8/10
            * 3 levels for distance: max distance is 50, 80/100, and 150/unlimited
        * Response variable is total abundance per PKEY (don't try to subset based on survey interval or band)
        * These two factors are additive, not interactive
        * Could include PCODE in addition to the above... to see if there are effects of Project independent of method

## Next Steps

* Clean up VRI data

